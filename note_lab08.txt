最想说的其实是 这实验怎么感觉和OS关系不是很大
有一种我刚刚写了个DSAA实验的感觉
或者说，试图改善一个程序在高并发时的表现，要从数据结构上找方法？
UPD 问了BBB，真的是这样

task1
    提升kalloc在面对大量并发访问时的性能

task1_sol
    先问为什么原先的kalloc很低性能
        原先的kmem用了一个list，然后list里装的是当前的free的PA，以Page为单位。
        由于list这个东西 我们显然希望从里取元素、往里面加元素的过程都是原子的，所以必须加锁
        在多核短时间大力并发kalloc的时候，他们会因为抢同一个spinlock而尬在那，非 常 低 效
    kfree同理，也是多个核心抢同一个锁的锅
    所以，我们没法用魔法使得一个链表能够支持并发的话，就得想办法让他不止有一个链表了。
    本实验就是让我们干这个事情，本来不是只有一个list吗，让多个核心来抢一个list好像不是很好，我们给他多弄点list。

    先看设计
        对于每个cpu，让他持有一个那个存储freePage的链表
        在该cpu需要一个page时，让他去霍霍他自己的list
        如果不够，再试图霍霍隔壁的list
    需要做的改动如下
        该该kalloc，别让所有cpu去抢同一个list
        改改freerange，使得free出来的pages跑到cpu对应的list里（其实不用改，但是要有这个想法
        改改kfree，从哪个cpu来的，就free到哪个cpu去
        在kalloc里，如果当前cpu的链表没东西了，就遍历一下cpu，看看其他cpu有没有空余页码，有的话偷一个过来
    在这样的设计下，每一个链表独自有个锁就好
    注意合理使用push_off关闭intr，不然乱玩cpuid会上天

task2
    提升blockCache对于高并发的性能

task2_sol
    还是先问blockCache为什么很低性能
        原先的blockCache的结构为双向链表，在链表里存储buffer节点
        在试图从Cache里找一个硬盘中的block时，我们只是单纯的大力遍历链表，然后看看有没有device和blockNumber均匹配的cache块儿存在
        听着就很tm暴力
        然后如果实在找不到，我们把LRU的块干掉，用来装这个
    显然双向链表的正确性依赖于操作的原子性，原子性依赖于锁
    所以面对高并发的时候，这东西显然没有性能。

    来看设计
        指导书教我们用哈希表来替换这个双向链表
        但是我感觉我实现了一个假的哈希表，有哈希，并没有链表。
        具体的，我开了mod数个桶，每个桶就是朴实无华一数组
        在找块儿的时候，去对应编号的桶里大力找
        如果没找到，那么把对应编号桶里LRU的给干掉，然后把对应的CacheBlock放进去
        由于没有写链表，不是很能照抄LRU，就按照指导书上的使用了时间戳战术
        （另外为什么我完全没打时间戳也能过啊西八，稳定干掉第一个块儿真的礼貌吗
        那么，在干掉块儿的时候，找到没有东西正在使用的，时间戳最小的块儿，干掉就完事
    在这样的设计下，每个哈希桶有个锁就行，并发性能显著提高。

    如果这么写了，会遇到一个panic: balloc: out of blocks
    百度搜索第一条告诉我们，在param.h里改一改FSSIZE就行
    是因为我的Cache太大了，原先的给Cache分配的空间装不下吗
    太魔鬼了

于是就是这样，在OS里完成了一个DSAA实验。
